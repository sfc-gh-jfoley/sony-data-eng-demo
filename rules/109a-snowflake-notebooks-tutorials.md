# Snowflake Notebook Tutorial Design Patterns

## Metadata

**SchemaVersion:** v3.2
**RuleVersion:** v3.0.0
**LastUpdated:** 2026-01-06
**Keywords:** checkpoints, learning objectives, pedagogical design, educational content, progressive learning, Snowflake notebooks, teaching point callouts, validation gates, tutorial structure, learning design, educational notebooks, teaching methodology, notebook education
**TokenBudget:** ~5250
**ContextTier:** High
**Depends:** 109-snowflake-notebooks.md, 920-data-science-analytics.md

## Scope

**What This Rule Covers:**
Comprehensive patterns for designing educational Snowflake notebooks that effectively teach concepts through clear learning objectives, structured content, anti-pattern examples, validation checkpoints, and progressive complexity management.

**When to Load This Rule:**
- Creating tutorial notebooks for Snowflake
- Designing educational content and learning materials
- Building self-paced learning experiences
- Structuring notebook-based training
- Implementing learning checkpoints and validation

## References

### External Documentation
- [Snowflake Notebooks Getting Started](https://docs.snowflake.com/en/user-guide/ui-snowsight-notebooks-gs) - Official quickstart guide
- [Snowpark for Python Tutorials](https://quickstarts.snowflake.com/guide/getting_started_with_snowpark_python/index.html) - Hands-on learning
- [Jupyter Best Practices](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html) - General notebook guidelines

### Related Rules
- **Notebook Core**: `109-snowflake-notebooks.md` - Core notebook patterns
- **App Deployment**: `109b-snowflake-app-deployment-core.md` - Production deployment
- **Snowflake Core**: `100-snowflake-core.md` - Foundational practices

## Contract

### Inputs and Prerequisites
- Technical notebook content from `109-snowflake-notebooks.md`
- Target audience definition (beginner/intermediate/advanced)
- Learning outcomes to achieve
- Time budget for tutorial completion

### Mandatory
- `edit_notebook` for adding educational cells
- `read_file` for reviewing existing content
- Markdown cells for narrative and teaching points

### Forbidden
- Overly technical jargon without definitions for beginner content
- Copy-paste code without explanation
- Examples without context or business rationale

### Execution Steps
1. Define clear learning objectives (3-6 outcomes)
2. Structure content into logical parts with time estimates
3. Add anti-pattern sections showing what NOT to do
4. Insert checkpoint validations between major sections
5. Include teaching point callouts explaining "why"
6. Provide progressive complexity (simple, then advanced)
7. Add self-assessment opportunities

### Post-Execution Checklist

- [ ] Learning objectives at cell #2 (3-6 measurable outcomes)
- [ ] Tutorial structure overview with time estimates
- [ ] Checkpoint validation cells between major sections
- [ ] Anti-patterns with both wrong and correct examples
- [ ] Teaching point callouts ([NOTE]) explaining "why"
- [ ] Progressive complexity (simple first, advanced marked)
- [ ] All jargon defined on first use

### Output Format
- Learning objectives section (markdown cell #2 after main header)
- Tutorial structure overview with time estimates
- Anti-pattern sections with incorrect and correct examples
- Checkpoint validation cells (code cells with assertions)
- Teaching point callouts ([NOTE] prefix in markdown)

### Validation
1. Verify learning objectives are measurable and clear
2. Confirm all major sections have checkpoint validations
3. Check anti-patterns have both incorrect and correct examples
4. Validate time estimates are realistic
5. Test self-paced learning flow (can skip ahead)

### Design Principles
- **Clear Learning Path:** Objectives, then Structure, then Checkpoints, then Assessment
- **Show Don't Tell:** Anti-patterns teach as effectively as best practices
- **Validate Progress:** Checkpoints prevent learners from proceeding with errors
- **Progressive Disclosure:** Simple concepts first, complexity builds gradually
- **Self-Paced Friendly:** Can skip sections, clear prerequisites stated
- **Context First:** Business rationale before technical implementation

## Anti-Patterns and Common Mistakes

**Anti-Pattern 1: Vague Learning Objectives**
```markdown
## Learning Objectives
1. Learn about ML
2. Understand data
3. Build models
```
**Problem:** Not measurable, no clear outcomes, too broad - learners won't know what specific skills they'll gain.

**Correct Pattern:**
```markdown
## [GOAL] Learning Objectives

By the end of this notebook, you will understand:

1. **Feature Engineering** - How to organize 16 ML features using Snowflake Feature Store
2. **Imbalanced Data Strategies** - Compare SMOTE vs Algorithm-level balancing approaches
3. **Threshold Optimization** - Tune decision thresholds to align with business objectives
```
**Benefits:** Specific, measurable outcomes with clear deliverables that learners can validate.

**Anti-Pattern 2: No Anti-Pattern Teaching**
```markdown
## Best Practices
- Use stratified splitting
- Apply SMOTE after split
```
**Problem:** Doesn't teach what to avoid or why - learners may still make common mistakes.

**Correct Pattern:**
```markdown
## Common Pitfalls
**Pitfall 1: Applying SMOTE Before Split**
- Why wrong: Data leakage (synthetic samples use test data)
- Correct: Split FIRST, then SMOTE only on training

**Pitfall 2: Using Accuracy for Imbalanced Data**
- Why wrong: 99% accuracy predicting all "healthy" catches zero failures
- Correct: Use recall, ROC-AUC, Matthews Correlation
```
**Benefits:** Teaches through contrast, explains consequences, prevents common errors.

## Post-Execution Checklist

- [ ] Learning objectives section present (3-6 outcomes, measurable, placed at cell #2)
- [ ] Tutorial structure overview with time estimates provided
- [ ] Anti-pattern sections with incorrect and correct examples (3-5 per topic)
- [ ] Checkpoint validation cells between major sections (3-7 checks each)
- [ ] Teaching point callouts ([NOTE] prefix) explaining "why" before implementations
- [ ] Progressive complexity (simple, then real-world, then advanced pattern)
- [ ] Self-paced friendly (can skip sections, prerequisites stated clearly)
- [ ] Two-approach clarifications when demonstrating but not fully using features
- [ ] Time estimates realistic and tested
- [ ] All technical jargon defined on first use

## Validation

- **Success Checks:**
  - Learning objectives are specific and measurable
  - Checkpoints catch common errors and provide actionable fixes
  - Anti-patterns include both wrong and correct approaches with explanations
  - Time estimates match actual execution (tested)
  - Teaching points explain business context, not just technical details
  - Progressive complexity builds logically
  - Notebook works for self-paced learning (can skip ahead with prerequisites met)

- **Negative Tests:**
  - Vague objectives ("learn about ML") should be flagged
  - Missing checkpoints should be added between major sections
  - Anti-patterns without correct alternatives should be enhanced
  - Technical jargon without definitions should trigger warnings
  - Overly complex introductory sections should be simplified

> **Investigation Required**
> When applying this rule:
> 1. **Read existing notebook cells BEFORE creating learning objectives** - Extract actual content taught (algorithms, techniques, data patterns)
> 2. **Count major sections for structure calculation** - Use for "Parts" organization (aim for 5-7 parts max, group related steps)
> 3. **Analyze code complexity to validate audience level** - Beginner: basic operations; Intermediate: algorithms + optimization; Advanced: custom implementations
> 4. **Estimate time based on cell count and complexity** - Formula: ~30 seconds per markdown cell (reading), ~60 seconds per code cell (reading + execution)
> 5. **Check for existing checkpoint patterns** - Identify validation gates present vs needed (add between major transitions)
> 6. **Verify technical prerequisites mentioned** - List required knowledge explicitly (e.g., "assumes familiarity with pandas DataFrames")
> 7. **Never create generic learning objectives** - Base on specific features/concepts actually demonstrated in notebook
>
> **Anti-Pattern:**
> "Based on typical ML notebooks, you probably want to teach feature engineering and model training..."
> "Most tutorials include these standard learning objectives..."
> [Creates generic objectives without reading notebook]
>
> **Correct Pattern:**
> "Let me read your notebook structure first."
> [reads notebook cells, counts sections, analyzes code complexity]
> "I found 8 major sections across 42 cells (18 markdown, 24 code). Code complexity: intermediate (uses scikit-learn pipeline, SMOTE, threshold tuning).
>
> Based on actual content:
> 1. **Feature Engineering** - Organizes 16 ML features using Snowflake Feature Store (cells 5-12)
> 2. **Imbalanced Data Strategies** - Compares SMOTE vs class_weight parameter (cells 15-24, demonstrates 4 models)
> 3. **Threshold Optimization** - Tunes decision thresholds using ROC curves to minimize missed failures (cells 28-35)
>
> Estimated time: 12-15 minutes full tutorial (5-7 minutes quick demo with n_estimators=50)"

## Output Format Examples

```sql
-- Analysis Query: Investigate current state
SELECT column_pattern, COUNT(*) as usage_count
FROM information_schema.columns
WHERE table_schema = 'TARGET_SCHEMA'
GROUP BY column_pattern;

-- Implementation: Apply Snowflake best practices
CREATE OR REPLACE VIEW schema.view_name
COMMENT = 'Business purpose following semantic model standards'
AS
SELECT
    -- Explicit column list with business context
    id COMMENT 'Surrogate key',
    name COMMENT 'Business entity name',
    created_at COMMENT 'Record creation timestamp'
FROM schema.source_table
WHERE is_active = TRUE;

-- Validation: Confirm implementation
SELECT * FROM schema.view_name LIMIT 5;
SHOW VIEWS LIKE '%view_name%';
```

## Learning Objectives Section

**MANDATORY:**

**Purpose:** Set clear expectations for what learners will achieve by completing the notebook.

**Structure:**
```markdown

## [GOAL] Learning Objectives

By the end of this notebook, you will understand:

1. **[Concept 1]** - [Specific skill or knowledge]
2. **[Concept 2]** - [Specific skill or knowledge]
3. **[Concept 3]** - [Specific skill or knowledge]
4. **[Concept 4]** - [Specific skill or knowledge]
5. **[Concept 5]** - [Specific skill or knowledge]
6. **[Concept 6]** - [Specific skill or knowledge]
```

**Best Practices:**
- **Requirement:** Place immediately after main header (cell #2)
- **Requirement:** 3-6 objectives maximum (cognitive load management)
- **Requirement:** Use action verbs (understand, implement, compare, optimize, analyze)
- **Requirement:** Each objective should be measurable and achievable
- **Always:** Bold the concept, explain the specific outcome
- **Consider:** Order from foundational to advanced concepts

**Example - Good:**
```markdown
1. **Feature Engineering** - How to organize 16 ML features using Snowflake Feature Store
2. **Imbalanced Data Strategies** - Compare SMOTE vs Algorithm-level balancing approaches
3. **Threshold Optimization** - Tune decision thresholds to align with business objectives
```

**Example - Bad:**
```markdown
1. Learn about features
2. Understand data
3. Build models
```
Too vague, not measurable, no clear outcome

## Tutorial Structure Overview

**MANDATORY:**

**Purpose:** Provide roadmap of tutorial organization with time estimates for self-paced learning.

**Structure:**
```markdown

## ðŸ“š Tutorial Structure

This notebook is organized into [N] parts:

**Part 1: [Name]** (Steps X-Y)
- [Brief description of what's covered]
- [Key concepts introduced]

**Part 2: [Name]** (Steps X-Y)
- [Brief description]
- [Key concepts]

...

## Estimated Time

- **Quick Demo Mode:** X-Y minutes ([configuration])
- **Full Tutorial Mode:** X-Y minutes ([configuration])
- **Production Mode:** X-Y minutes ([configuration])

*Configure scenario in Step [reference]*
```

**Best Practices:**
- **Requirement:** Place after Learning Objectives (cell #3)
- **Requirement:** Provide time estimates for different modes
- **Always:** Group related steps into logical parts (5-7 parts maximum)
- **Always:** Indicate what each part teaches
- **Consider:** Show dependencies between parts (can skip if prerequisites known)

## Anti-Pattern Sections

**MANDATORY:**

**Purpose:** Teach what NOT to do by showing common mistakes alongside correct approaches.

**Structure:**
```markdown

## Common Pitfalls in [Topic]

Understanding what NOT to do is as important as knowing best practices:

**Pitfall 1: [Descriptive Name]**
- Why wrong: [Explanation of the problem]
- Correct: [What to do instead]

**Pitfall 2: [Descriptive Name]**
- Why wrong: [Explanation]
- Correct: [Correct approach]

**Pitfall 3: [Descriptive Name]**
- Why wrong: [Explanation]
- Correct: [Correct approach]

This notebook avoids all these pitfalls through careful design!
```

**Best Practices:**
- **Requirement:** 3-5 pitfalls per major topic
- **Requirement:** Always pair wrong with correct
- **Always:** Explain WHY it's wrong, not just WHAT is wrong
- **Always:** Show concrete impact (performance, cost, accuracy)
- **Consider:** Reference where in notebook the correct pattern is demonstrated

**Example - Good:**
```markdown
**Pitfall 1: Applying SMOTE Before Train/Test Split**
- Why wrong: Synthetic samples use information from test set (data leakage)
- Correct: Split FIRST, then apply SMOTE only to training data

**Pitfall 2: Using Accuracy for Imbalanced Data**
- Why wrong: 99.998% accuracy by predicting all healthy catches zero failures
- Correct: Use recall, ROC-AUC, Matthews Correlation, PR curve
```

## Checkpoint Validations

**MANDATORY:**

**Purpose:** Automated validation gates that verify learner progress and prevent proceeding with errors.

**Structure:**
```markdown

## Checkpoint N - [Name] Complete

Before proceeding to [next section], verify all [previous section] steps succeeded.
```

```python
# Checkpoint [N] Validation
print("=" * 80)
print("[PASS] CHECKPOINT [N]: [NAME]")
print("=" * 80)

checks_passed = []
checks_failed = []

# Check 1: [Description]
if [condition]:
    checks_passed.append("[PASS] [Success message]")
else:
    checks_failed.append("[Failure message] - run Step X.Y")

# Check 2: [Description]
if [condition]:
    checks_passed.append("[PASS] [Success message]")
else:
    checks_failed.append("[Failure message] - run Step X.Y")

# Display results
print("\nValidation Results:")
print("-" * 80)
for check in checks_passed:
    print(check)

if checks_failed:
    print("\nIssues Detected:")
    for check in checks_failed:
        print(check)
    print("\nFix issues above before proceeding to [next section]")
    print("=" * 80)
else:
    print("\nALL CHECKS PASSED - Ready for [next section]!")
    print("=" * 80)
    print("\nNext Steps:")
    print("  - [Description of what comes next]")
```

**Best Practices:**
- **Requirement:** Place checkpoints between major sections (not every cell)
- **Requirement:** 3-7 validation checks per checkpoint
- **Always:** Check for critical state (data loaded, models trained, features present)
- **Always:** Provide actionable error messages (which step to re-run)
- **Always:** Show progress summary (what's complete, what's next)
- **Consider:** Include diagnostic information (row counts, feature counts, time elapsed)

## Teaching Point Callouts

**MANDATORY:**

**Purpose:** Inline explanations of WHY decisions were made, providing context and rationale.

**Structure:**
```markdown
### [NOTE] Teaching Point: [Topic]

**[Key Concept/Question]:**
- [Explanation point 1]
- [Explanation point 2]
- [Explanation point 3]

**Why This Matters:**
- [Business or technical impact]
- [Cost/performance/reliability consideration]

**[Comparison/Strategy]:**
1. **Approach A:** [Description and tradeoffs]
2. **Approach B:** [Description and tradeoffs]

**Demo Strategy:** [How this notebook demonstrates the concept]
```

**Best Practices:**
- **Requirement:** Use [NOTE] emoji prefix for visual scanning
- **Always:** Place BEFORE the implementation (context before code)
- **Always:** Explain business rationale, not just technical details
- **Consider:** Use tables for comparing approaches
- **Consider:** Reference external documentation for deeper learning

**Example - Good:**
```markdown
### [NOTE] Teaching Point: Why Class Imbalance Matters

**The Real-World Problem:**
- In production datasets, failures are rare (often <5% of samples)
- Standard ML algorithms optimize for overall accuracy
- Result: Model predicts "healthy" for everything, achieving 95% accuracy but 0% recall!

**Why This Fails in Practice:**
- **Business Cost Asymmetry:** Missed failure = $100,000+ in emergency repairs
- **False alarm cost:** $1,000 for planned inspection
- **Cost ratio:** 100:1 (some utilities report 10-50x)

**Two Solutions to Explore:**
1. **Path A (SMOTE):** Create synthetic failure samples to balance training data
2. **Path B (Algorithm-Level):** Use algorithms that internally handle imbalance

**Demo Strategy:** We'll train 4 models (2 from each path) and compare results.
```

## Progressive Complexity Management

**MANDATORY:**

**Purpose:** Gradually increase complexity, building on foundational concepts before introducing advanced topics.

**Pattern:**
1. **Simple Example:** Minimal viable implementation
2. **Real-World Context:** Why simple example isn't sufficient
3. **Production Pattern:** Complete implementation with error handling
4. **Advanced Optimization:** Performance/cost improvements

**Best Practices:**
- **Requirement:** Start with simplest working example
- **Always:** Explain what each layer of complexity adds
- **Always:** Mark optional advanced sections clearly
- **Consider:** Provide "skip to advanced" shortcuts for experienced learners
- **Consider:** Use configuration variables to toggle complexity (quick_demo vs production)

**Example Structure:**
```markdown

## Step 1 - Basic Implementation (Simple)
[Minimal code that works]

## Step 2 - Production Considerations (Real-World)
[Add error handling, validation, logging]

## Step 3 - Performance Optimization (Advanced - Optional)
[Caching, parallelization, cost optimization]
```

## Two-Approach Clarification Pattern

**MANDATORY:**

**Purpose:** When notebook demonstrates feature but uses simplified approach, explain BOTH approaches and WHY the simpler one is used.

**Structure:**
```markdown

## [NOTE] [Feature]: Two Approaches Explained

**Why did we [setup feature] but not use it?**

The notebook demonstrates **two valid approaches** for [task]:

### Approach A: [Production Name] âœ¨
```[language]
[Code example]
```

**Benefits:**
- [Benefit 1]
- [Benefit 2]
- [Benefit 3]

**Use when:** [Scenario]

### Approach B: [Simplified Name]
```[language]
[Code example]
```

**Benefits:**
- [Benefit 1]
- [Benefit 2]
- [Benefit 3]

**Use when:** [Scenario]

### This Notebook's Approach

**We use Approach B** to keep the focus on [primary learning goal]. The [feature] setup in Steps X-Y shows you **how to organize for production** while using the simpler approach for actual training.

**For production deployments:** Uncomment the [feature] code in Step [X] and use `[method]()`.

**Learning Takeaway:** [Key insight about when to use each approach]
```

**Best Practices:**
- **Requirement:** Use when demonstrating feature but not fully utilizing it
- **Always:** Explain why BOTH approaches are valid
- **Always:** Clarify which approach the notebook uses and WHY
- **Always:** Provide guidance on when to use production approach

### Real-World Example: Feature Store Setup

**Scenario:** Notebook demonstrates Feature Store entity/feature organization but trains models using simplified DataFrame approach.

````markdown

## [NOTE] Feature Store: Two Approaches Explained

**Why did we set up Feature Store entities but not use `fs.generate_dataset()`?**

The notebook demonstrates **two valid approaches** for feature engineering:

### Approach A: Feature Store `generate_dataset()` âœ¨
```python
# Production-ready: Automatic joins, time-travel, lineage tracking
training_df = fs.generate_dataset(
    spine_df=spine,
    features=[customer_features, order_features],
    spine_timestamp_col='OBSERVATION_DATE'
)
```

**Benefits:**
- Automatic feature joins across entities (no manual merge logic required)
- Point-in-time correctness (prevents data leakage in temporal scenarios)
- Lineage tracking (see which features used in which models via Feature Store UI)
- Version control (track feature definitions and transformations over time)

**Use when:** Multi-entity features (10+ features), production deployment scenarios, team collaboration with shared feature definitions

### Approach B: Direct Snowpark DataFrame Operations
```python
# Explicit joins for learning purposes
training_df = spine.join(customer_features, on='CUSTOMER_ID') \
                   .join(order_features, on='ORDER_ID')
```

**Benefits:**
- Transparent logic (see exactly how features combine in explicit joins)
- Simpler for single-entity features (no Feature Store overhead)
- Easier to debug for beginners (direct DataFrame operations, familiar pandas-like API)
- Faster iteration for exploratory analysis (no entity registration needed)

**Use when:** Learning Feature Store concepts, exploratory analysis with <10 features, single-entity scenarios, rapid prototyping

### This Notebook's Approach

**We use Approach B** to keep the focus on ML algorithms and imbalanced data strategies. The Feature Store setup in Steps 2-3 shows you **how to organize for production** (entity registration, feature definitions) while using the simpler DataFrame approach for actual training.

**For production deployments:** Uncomment the `fs.generate_dataset()` code in Step 4 and use `training_df = fs.retrieve_feature_values(spine_df=spine, features=[...])` for automatic joins and lineage tracking.

**Learning Takeaway:** Feature Store adds governance and automation valuable for production multi-entity scenarios. Use it when team collaboration, lineage tracking, and point-in-time correctness matter. Use direct DataFrames for exploration, learning, and single-entity feature engineering.
````

This example demonstrates when to show feature setup (teaching organizational patterns) while using simpler execution (maintaining focus on primary learning objectives like imbalanced data handling).

## Learning Objectives
1. Learn about ML
2. Understand data
3. Build models
```
**Problem:** Not measurable, no clear outcomes, too broad

**Correct Pattern:**
```markdown

## Best Practices
- Use stratified splitting
- Apply SMOTE after split
```
**Problem:** Doesn't teach what to avoid or why

**Correct Pattern:**
```markdown

## Common Pitfalls
**Pitfall 1: Applying SMOTE Before Split**
- Why wrong: Data leakage (synthetic samples use test data)
- Correct: Split FIRST, then SMOTE only on training

**Pitfall 2: Using Accuracy for Imbalanced Data**
- Why wrong: 99% accuracy predicting all "healthy" catches zero failures
- Correct: Use recall, ROC-AUC, Matthews Correlation
```
**Benefits:** Teaches through contrast, explains consequences

**Anti-Pattern 4: No Time Estimates**
```markdown

## Tutorial Structure
Part 1: Setup
Part 2: Training
Part 3: Evaluation
```
**Problem:** Learners don't know time commitment

**Correct Pattern:**
```markdown

## Checkpoint 1 - [Name] Complete

```python
# Checkpoint 1 Validation
checks_passed = []
checks_failed = []

if [condition]:
    checks_passed.append("[PASS] [Check passed]")
else:
    checks_failed.append("[Check failed] - run Step X")

if checks_failed:
    print("Fix issues before proceeding")
else:
    print("ALL CHECKS PASSED")
```

### [NOTE] Teaching Point: [Topic]

**[Key Concept]:**
- [Explanation]

**Why This Matters:**
- [Business impact]
```
